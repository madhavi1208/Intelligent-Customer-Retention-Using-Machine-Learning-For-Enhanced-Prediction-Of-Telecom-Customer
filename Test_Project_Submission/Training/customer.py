# -*- coding: utf-8 -*-
"""customer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jMLHLVa9qpX_P011mgE5BmM6atzpshkn
"""

# Commented out IPython magic to ensure Python compatibility.
#import necessary libraries
import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import sklearn
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import RandomizedSearchCV 
import imblearn
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report,confusion_matrix,f1_score

#import dataset
data=pd.read_csv(r"/content/Telco_Cust_Churn.csv")
data

data.info()

data.TotalCharges = pd.to_numeric(data.TotalCharges,errors='coerce')
data.isnull().any()

data["TotalCharges"].fillna(data["TotalCharges"].median(),inplace=True)
data.isnull().sum()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
data["gender"]=le.fit_transform(data["gender"])
data["Partner"] = le.fit_transform(data["Partner"])
data["Dependents"] = le.fit_transform(data["Dependents"])
data["PhoneService"] = le.fit_transform(data["PhoneService"])
data["MultipleLines"] = le.fit_transform(data["MultipleLines"])
data["InternetService"] = le.fit_transform(data["InternetService"])
data["OnlineSecurity"] = le.fit_transform(data["OnlineSecurity"])
data["OnlineBackup"] = le.fit_transform(data["OnlineBackup"])
data["DeviceProtection"] = le.fit_transform(data["DeviceProtection"])
data["TechSupport"] = le.fit_transform(data["TechSupport"])
data["StreamingTV"] = le.fit_transform(data["StreamingTV"])
data["StreamingMovies"] = le.fit_transform(data["StreamingMovies"])
data["Contract"] = le.fit_transform(data["Contract"])
data["PaperlessBilling"] = le.fit_transform(data["PaperlessBilling"])
data["PaymentMethod"] = le.fit_transform(data["PaymentMethod"])
data["Churn"] = le.fit_transform(data["Churn"])

data.head()

x=data.iloc[:,1:20].values
y=data.iloc[:,20:21].values

x

y

from sklearn.preprocessing import OneHotEncoder
one=OneHotEncoder()
a= one.fit_transform(x[:,6:7]).toarray()
b= one.fit_transform(x[:,7:8]).toarray()
c= one.fit_transform(x[:,8:9]).toarray()
d= one.fit_transform(x[:,9:10]).toarray()
e= one.fit_transform(x[:,10:11]).toarray()
f= one.fit_transform(x[:,11:12]).toarray()
g= one.fit_transform(x[:,12:13]).toarray()
h= one.fit_transform(x[:,13:14]).toarray()
i= one.fit_transform(x[:,14:15]).toarray()
j= one.fit_transform(x[:,16:17]).toarray()
x=np.delete(x,[6,7,8,9,10,11,12,13,14,16],axis=1)
x=np.concatenate((a,b,c,d,e,f,g,h,i,j,x),axis=1)

from imblearn.over_sampling import SMOTE

smt=SMOTE()
x_resample,y_resample = smt.fit_resample(x,y)

x_resample

y_resample

x.shape,x_resample.shape

y.shape,y_resample.shape

data.describe()

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
sns.distplot(data["tenure"])
plt.subplot(1,2,2)
sns.distplot(data["MonthlyCharges"])

import matplotlib.pyplot as plt  
import seaborn as sns  
import pandas as pd  
#loading the dataset'tips'  
df=pd.read_csv( r"/content/Telco_Cust_Churn.csv")
#plotting the graph  
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
sns.countplot(x='gender',data=df) 
plt.subplot(1,2,2)
sns.countplot(x='Dependents',data=df) 
plt.show()

sns.barplot(x="Churn",y="MonthlyCharges",data=data)

sns.heatmap(data.corr(),annot=True)

sns.pairplot(data=data, markers=["^","v"], palette="inferno")

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x_resample,y_resample,test_size=0.2,random_state=0)

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.fit_transform(x_test)

x_train.shape

#importig and building the Decision tree model
def logreg(x_train,x_test,y_train,y_test):
  lr = LogisticRegression(random_state=0)
  lr.fit(x_train,y_train)
  y_lr_tr = lr.predict(x_train)
  print(accuracy_score(y_lr_tr,y_train))
  yPred_lr = lr.predict(x_test)
  print(accuracy_score(yPred_lr,y_test))
  print("***Logistic Regression***")
  print("Confusion_Matrix")
  print(confusion_matrix(y_test,yPred_lr))
  print("Classification Report")
  print(classification_report(y_test,yPred_lr))

#printing the train accuracy and test accuracy respectively
logreg(x_train,x_test,y_train,y_test)

#importing and building the Decision tree model
def decisionTree(x_train,x_test,y_train,y_test):
   dtc =DecisionTreeClassifier(criterion="entropy",random_state=0)
   dtc.fit(x_train,y_train)
   y_dt_tr = dtc.predict(x_train)
   print(accuracy_score(y_dt_tr,y_train))
   yPred_dt = dtc.predict(x_test)
   print(accuracy_score(yPred_dt,y_test))
   print("***Decision Tree***")
   print("confusion_Matrix")
   print(confusion_matrix(y_test,yPred_dt))
   print("Classification Report")
   print(classification_report(y_test,yPred_dt))

#printing the train accuracy and test accuracy respectively
decisionTree(x_train,x_test,y_train,y_test)

#importing and building the random forest model
def RandomForest(x_train,x_test,y_train,y_test):
   rf = RandomForestClassifier(criterion="entropy",n_estimators=10,random_state=0)
   rf.fit(x_train,y_train)
   y_rf_tr = rf.predict(x_train)
   print(accuracy_score(y_rf_tr,y_train))
   yPred_rf = rf.predict(x_test)
   print(accuracy_score(yPred_rf,y_test))
   print("***Random Forest")
   print("Confusion_Matrix")
   print(confusion_matrix(y_test,yPred_rf))
   print("Classification Report")
   print(classification_report(y_test,yPred_rf))

#printing the train accurancy and test accurancy and test accurancy respectively
RandomForest(x_train,x_test,y_train,y_test)

#importing and building the KNN model
def KNN(x_tarin,x_test,y_train,y_test):
   knn = KNeighborsClassifier()
   knn.fit(x_train,y_train)
   y_knn_tr = knn.predict(x_train)
   print(accuracy_score(y_knn_tr,y_train))
   yPred_knn = knn.predict(x_test)
   print(accuracy_score(yPred_knn,y_test))
   print("***KNN***")
   print("Confusion_Matrix")
   print(confusion_matrix(y_test,yPred_knn))
   print("Classification Report")
   print(classification_report(y_test,yPred_knn))

#printing the train accuracy and test accuracy respectively
KNN(x_train,x_test,y_train,y_test)

#importing and building the random forest model
def SVM(x_train,x_test,y_train,y_test):
  SVM = SVC(kernel = "linear")
  SVM.fit(x_train,y_train)
  y_svm_tr = SVM.predict(x_train)
  print(accuracy_score(y_svm_tr,y_train))
  yPred_svm = SVM.predict(x_test)
  print(accuracy_score(yPred_svm,y_test))
  print("***Support Vector Machine***")
  print("Confusion_Matrix")
  print(confusion_matrix(y_test,yPred_svm))
  print("Classification Report")
  print(classification_report(y_test,yPred_svm))

#printing the tarin accuracy and test accuracy respectively
SVM(x_train,x_test,y_train,y_test)

#importing the train Keras libraries and packages
import keras
from keras.models import Sequential
from keras.layers import Dense

#Initialising the ANN
classifier = Sequential()

#Adding the input layer and the first hidden layer
classifier.add(Dense(units=30,activation='relu',input_dim=40))

#Adding the second hidden layer
classifier.add(Dense(units=30,activation='relu'))

#Adding the output layer
classifier.add(Dense(units=1,activation='sigmoid'))

#Compiling theANN
classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

#Fitting the ANN in the Training set
model_history = classifier.fit(x_train,y_train, batch_size=10,validation_split=0.33, epochs=200)

ann_pred =classifier.predict(x_test)
ann_pred =(ann_pred>0.5)
ann_pred

print(accuracy_score(ann_pred,y_test))
print("***ANN Model***")
print("Confusion_Matrix")
print(confusion_matrix(y_test,ann_pred))
print("Classiification Report")
print(classification_report(y_test,ann_pred))

#testing on random input values
lr = LogisticRegression(random_state=0)
lr.fit(x_train,y_train)
print("Predicting on random input")
lr_pred_own = lr.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]]))
print("output is:", lr_pred_own)

#testing on random input values
dtc = DecisionTreeClassifier(criterion="entropy",random_state=0)
dtc.fit(x_train,y_train)
print("Predicting on random input")
dtc_pred_own = dtc.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3425,4567]]))
print("output is: ",dtc_pred_own)

#testing on random input values
rf = RandomForestClassifier(criterion="entropy",n_estimators=10,random_state=0)
rf.fit(x_train,y_train)
print("Prediction on random input")
rf_pred_own = rf.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]]))
print("output is: ",rf_pred_own)

#testing on random input values
svc = SVC(kernel = "linear")
svc.fit(x_train,y_train)
print("Prediction on random input")
svm_pred_own = svc.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]]))
print("output is:",svm_pred_own)

#testing on random input values
knn = KNeighborsClassifier()
knn.fit(x_train,y_train)
print("Prediction on random input")
knn_pred_own = knn.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]]))
print("output is: ",knn_pred_own)

def compareModel(x_train,x_test,y_train,y_test):
  logreg(x_train,x_test,y_train,y_test)
  print('-'*100)
  decisionTree(x_train,x_test,y_train,y_test)
  print('-'*100)
  RandomForest(x_train,x_test,y_train,y_test)
  print('-'*100)
  SVM(x_train,x_test,y_train,y_test)
  print('-'*100)
  KNN(x_train,x_test,y_train,y_test)
  print('-'*100)

compareModel(x_train,x_test,y_train,y_test)

print(accuracy_score(ann_pred,y_test))
print("***ANN Model***")
print("Confusion_Matrix")
print(confusion_matrix(y_test,ann_pred))
print("Classification Report")
print(classification_report(y_test,ann_pred))

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# define the random forest classifier model
model = RandomForestClassifier()

# define the hyperparameters to tune
params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# perform grid search cross-validation to find the best hyperparameters
grid_search = GridSearchCV(model, params, cv=5)
grid_search.fit(x_train, y_train)

# print the best hyperparameters found by grid search
print("Best hyperparameters:", grid_search.best_params_)

# get the best model from grid search
model = grid_search.best_estimator_

# evaluate the model on the training set
y_rf = model.predict(x_train)
print("Training set accuracy:", accuracy_score(y_rf, y_train))

# evaluate the model on the test set
yPred_rfcv = model.predict(x_test)
print("Test set accuracy:", accuracy_score(yPred_rfcv, y_test))

# print the confusion matrix and classification report for the test set
print("**Random Forest after Hyperparameter tuning**")
print("Confusion Matrix")
print(confusion_matrix(y_test, yPred_rfcv))
print("Classification Report")
print(classification_report(y_test, yPred_rfcv))

# use the model to predict on a new input
rfcv_pred_own = model.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]]))
print("Output is:", rfcv_pred_own)

classifier.save("telcom_churn.h5")